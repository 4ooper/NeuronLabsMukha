{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5OwpZfFhG8_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33013587-8046-4448-cda6-3e763d3d9193"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 loss: 0.750\n",
            "w1: -0.735\n",
            "w2: -0.644\n",
            "w3: -0.496\n",
            "w4: 1.088\n",
            "w5: -0.163\n",
            "w6: 2.299\n",
            "Epoch 2 loss: 0.750\n",
            "w1: -0.734\n",
            "w2: -0.641\n",
            "w3: -0.514\n",
            "w4: 1.045\n",
            "w5: -0.203\n",
            "w6: 2.165\n",
            "Epoch 3 loss: 0.750\n",
            "w1: -0.733\n",
            "w2: -0.636\n",
            "w3: -0.536\n",
            "w4: 0.989\n",
            "w5: -0.254\n",
            "w6: 1.999\n",
            "Epoch 4 loss: 0.750\n",
            "w1: -0.731\n",
            "w2: -0.628\n",
            "w3: -0.559\n",
            "w4: 0.915\n",
            "w5: -0.317\n",
            "w6: 1.793\n",
            "Epoch 5 loss: 0.750\n",
            "w1: -0.731\n",
            "w2: -0.614\n",
            "w3: -0.580\n",
            "w4: 0.824\n",
            "w5: -0.394\n",
            "w6: 1.556\n",
            "Epoch 6 loss: 0.750\n",
            "w1: -0.733\n",
            "w2: -0.595\n",
            "w3: -0.590\n",
            "w4: 0.732\n",
            "w5: -0.472\n",
            "w6: 1.322\n",
            "Epoch 7 loss: 0.250\n",
            "w1: -0.739\n",
            "w2: -0.575\n",
            "w3: -0.591\n",
            "w4: 0.657\n",
            "w5: -0.539\n",
            "w6: 1.135\n",
            "Epoch 8 loss: 0.250\n",
            "w1: -0.747\n",
            "w2: -0.556\n",
            "w3: -0.586\n",
            "w4: 0.601\n",
            "w5: -0.588\n",
            "w6: 1.000\n",
            "Epoch 9 loss: 0.250\n",
            "w1: -0.756\n",
            "w2: -0.538\n",
            "w3: -0.577\n",
            "w4: 0.559\n",
            "w5: -0.622\n",
            "w6: 0.902\n",
            "Epoch 10 loss: 0.250\n",
            "w1: -0.767\n",
            "w2: -0.523\n",
            "w3: -0.568\n",
            "w4: 0.526\n",
            "w5: -0.648\n",
            "w6: 0.830\n",
            "Epoch 11 loss: 0.250\n",
            "w1: -0.778\n",
            "w2: -0.508\n",
            "w3: -0.558\n",
            "w4: 0.498\n",
            "w5: -0.667\n",
            "w6: 0.772\n",
            "Epoch 12 loss: 0.250\n",
            "w1: -0.789\n",
            "w2: -0.495\n",
            "w3: -0.548\n",
            "w4: 0.475\n",
            "w5: -0.683\n",
            "w6: 0.726\n",
            "Epoch 13 loss: 0.250\n",
            "w1: -0.801\n",
            "w2: -0.482\n",
            "w3: -0.538\n",
            "w4: 0.455\n",
            "w5: -0.695\n",
            "w6: 0.687\n",
            "Epoch 14 loss: 0.250\n",
            "w1: -0.813\n",
            "w2: -0.470\n",
            "w3: -0.529\n",
            "w4: 0.436\n",
            "w5: -0.706\n",
            "w6: 0.654\n",
            "Epoch 15 loss: 0.250\n",
            "w1: -0.824\n",
            "w2: -0.458\n",
            "w3: -0.519\n",
            "w4: 0.419\n",
            "w5: -0.716\n",
            "w6: 0.625\n",
            "Epoch 16 loss: 0.250\n",
            "w1: -0.836\n",
            "w2: -0.446\n",
            "w3: -0.510\n",
            "w4: 0.404\n",
            "w5: -0.724\n",
            "w6: 0.600\n",
            "Epoch 17 loss: 0.250\n",
            "w1: -0.848\n",
            "w2: -0.435\n",
            "w3: -0.501\n",
            "w4: 0.390\n",
            "w5: -0.732\n",
            "w6: 0.577\n",
            "Epoch 18 loss: 0.250\n",
            "w1: -0.860\n",
            "w2: -0.423\n",
            "w3: -0.492\n",
            "w4: 0.376\n",
            "w5: -0.739\n",
            "w6: 0.556\n",
            "Epoch 19 loss: 0.250\n",
            "w1: -0.872\n",
            "w2: -0.412\n",
            "w3: -0.484\n",
            "w4: 0.363\n",
            "w5: -0.746\n",
            "w6: 0.537\n",
            "Epoch 20 loss: 0.250\n",
            "w1: -0.884\n",
            "w2: -0.401\n",
            "w3: -0.476\n",
            "w4: 0.351\n",
            "w5: -0.753\n",
            "w6: 0.519\n",
            "Epoch 21 loss: 0.250\n",
            "w1: -0.897\n",
            "w2: -0.390\n",
            "w3: -0.468\n",
            "w4: 0.340\n",
            "w5: -0.759\n",
            "w6: 0.503\n",
            "Epoch 22 loss: 0.250\n",
            "w1: -0.909\n",
            "w2: -0.379\n",
            "w3: -0.460\n",
            "w4: 0.328\n",
            "w5: -0.766\n",
            "w6: 0.488\n",
            "Epoch 23 loss: 0.250\n",
            "w1: -0.921\n",
            "w2: -0.368\n",
            "w3: -0.452\n",
            "w4: 0.318\n",
            "w5: -0.773\n",
            "w6: 0.473\n",
            "Epoch 24 loss: 0.250\n",
            "w1: -0.933\n",
            "w2: -0.356\n",
            "w3: -0.445\n",
            "w4: 0.307\n",
            "w5: -0.780\n",
            "w6: 0.460\n",
            "Epoch 25 loss: 0.250\n",
            "w1: -0.945\n",
            "w2: -0.345\n",
            "w3: -0.438\n",
            "w4: 0.297\n",
            "w5: -0.786\n",
            "w6: 0.447\n",
            "Epoch 26 loss: 0.250\n",
            "w1: -0.957\n",
            "w2: -0.334\n",
            "w3: -0.431\n",
            "w4: 0.287\n",
            "w5: -0.794\n",
            "w6: 0.434\n",
            "Epoch 27 loss: 0.250\n",
            "w1: -0.969\n",
            "w2: -0.322\n",
            "w3: -0.424\n",
            "w4: 0.278\n",
            "w5: -0.801\n",
            "w6: 0.423\n",
            "Epoch 28 loss: 0.250\n",
            "w1: -0.981\n",
            "w2: -0.311\n",
            "w3: -0.418\n",
            "w4: 0.269\n",
            "w5: -0.809\n",
            "w6: 0.411\n",
            "Epoch 29 loss: 0.250\n",
            "w1: -0.993\n",
            "w2: -0.299\n",
            "w3: -0.411\n",
            "w4: 0.260\n",
            "w5: -0.816\n",
            "w6: 0.401\n",
            "Epoch 30 loss: 0.250\n",
            "w1: -1.006\n",
            "w2: -0.287\n",
            "w3: -0.405\n",
            "w4: 0.251\n",
            "w5: -0.825\n",
            "w6: 0.390\n",
            "Epoch 31 loss: 0.250\n",
            "w1: -1.018\n",
            "w2: -0.275\n",
            "w3: -0.399\n",
            "w4: 0.242\n",
            "w5: -0.833\n",
            "w6: 0.381\n",
            "Epoch 32 loss: 0.250\n",
            "w1: -1.030\n",
            "w2: -0.263\n",
            "w3: -0.393\n",
            "w4: 0.234\n",
            "w5: -0.842\n",
            "w6: 0.371\n",
            "Epoch 33 loss: 0.250\n",
            "w1: -1.042\n",
            "w2: -0.250\n",
            "w3: -0.388\n",
            "w4: 0.226\n",
            "w5: -0.851\n",
            "w6: 0.362\n",
            "Epoch 34 loss: 0.250\n",
            "w1: -1.054\n",
            "w2: -0.238\n",
            "w3: -0.382\n",
            "w4: 0.218\n",
            "w5: -0.860\n",
            "w6: 0.353\n",
            "Epoch 35 loss: 0.250\n",
            "w1: -1.066\n",
            "w2: -0.225\n",
            "w3: -0.377\n",
            "w4: 0.210\n",
            "w5: -0.870\n",
            "w6: 0.344\n",
            "Epoch 36 loss: 0.250\n",
            "w1: -1.079\n",
            "w2: -0.212\n",
            "w3: -0.371\n",
            "w4: 0.202\n",
            "w5: -0.880\n",
            "w6: 0.336\n",
            "Epoch 37 loss: 0.250\n",
            "w1: -1.091\n",
            "w2: -0.199\n",
            "w3: -0.366\n",
            "w4: 0.195\n",
            "w5: -0.890\n",
            "w6: 0.328\n",
            "Epoch 38 loss: 0.250\n",
            "w1: -1.103\n",
            "w2: -0.186\n",
            "w3: -0.361\n",
            "w4: 0.188\n",
            "w5: -0.901\n",
            "w6: 0.321\n",
            "Epoch 39 loss: 0.250\n",
            "w1: -1.116\n",
            "w2: -0.172\n",
            "w3: -0.357\n",
            "w4: 0.181\n",
            "w5: -0.912\n",
            "w6: 0.313\n",
            "Epoch 40 loss: 0.250\n",
            "w1: -1.128\n",
            "w2: -0.158\n",
            "w3: -0.352\n",
            "w4: 0.174\n",
            "w5: -0.923\n",
            "w6: 0.306\n",
            "Epoch 41 loss: 0.250\n",
            "w1: -1.140\n",
            "w2: -0.144\n",
            "w3: -0.347\n",
            "w4: 0.167\n",
            "w5: -0.935\n",
            "w6: 0.300\n",
            "Epoch 42 loss: 0.250\n",
            "w1: -1.153\n",
            "w2: -0.130\n",
            "w3: -0.343\n",
            "w4: 0.160\n",
            "w5: -0.947\n",
            "w6: 0.293\n",
            "Epoch 43 loss: 0.250\n",
            "w1: -1.165\n",
            "w2: -0.115\n",
            "w3: -0.339\n",
            "w4: 0.154\n",
            "w5: -0.959\n",
            "w6: 0.287\n",
            "Epoch 44 loss: 0.250\n",
            "w1: -1.177\n",
            "w2: -0.100\n",
            "w3: -0.335\n",
            "w4: 0.147\n",
            "w5: -0.972\n",
            "w6: 0.281\n",
            "Epoch 45 loss: 0.250\n",
            "w1: -1.190\n",
            "w2: -0.085\n",
            "w3: -0.331\n",
            "w4: 0.141\n",
            "w5: -0.985\n",
            "w6: 0.275\n",
            "Epoch 46 loss: 0.250\n",
            "w1: -1.202\n",
            "w2: -0.070\n",
            "w3: -0.327\n",
            "w4: 0.135\n",
            "w5: -0.999\n",
            "w6: 0.269\n",
            "Epoch 47 loss: 0.250\n",
            "w1: -1.215\n",
            "w2: -0.054\n",
            "w3: -0.323\n",
            "w4: 0.128\n",
            "w5: -1.012\n",
            "w6: 0.264\n",
            "Epoch 48 loss: 0.250\n",
            "w1: -1.227\n",
            "w2: -0.038\n",
            "w3: -0.319\n",
            "w4: 0.122\n",
            "w5: -1.027\n",
            "w6: 0.259\n",
            "Epoch 49 loss: 0.250\n",
            "w1: -1.240\n",
            "w2: -0.022\n",
            "w3: -0.315\n",
            "w4: 0.117\n",
            "w5: -1.041\n",
            "w6: 0.254\n",
            "Epoch 50 loss: 0.250\n",
            "w1: -1.253\n",
            "w2: -0.005\n",
            "w3: -0.312\n",
            "w4: 0.111\n",
            "w5: -1.056\n",
            "w6: 0.250\n",
            "Epoch 51 loss: 0.250\n",
            "w1: -1.265\n",
            "w2: 0.012\n",
            "w3: -0.308\n",
            "w4: 0.105\n",
            "w5: -1.072\n",
            "w6: 0.246\n",
            "Epoch 52 loss: 0.250\n",
            "w1: -1.278\n",
            "w2: 0.029\n",
            "w3: -0.305\n",
            "w4: 0.099\n",
            "w5: -1.087\n",
            "w6: 0.242\n",
            "Epoch 53 loss: 0.250\n",
            "w1: -1.291\n",
            "w2: 0.047\n",
            "w3: -0.302\n",
            "w4: 0.094\n",
            "w5: -1.104\n",
            "w6: 0.238\n",
            "Epoch 54 loss: 0.250\n",
            "w1: -1.303\n",
            "w2: 0.065\n",
            "w3: -0.299\n",
            "w4: 0.088\n",
            "w5: -1.120\n",
            "w6: 0.234\n",
            "Epoch 55 loss: 0.250\n",
            "w1: -1.316\n",
            "w2: 0.083\n",
            "w3: -0.295\n",
            "w4: 0.083\n",
            "w5: -1.137\n",
            "w6: 0.231\n",
            "Epoch 56 loss: 0.250\n",
            "w1: -1.329\n",
            "w2: 0.102\n",
            "w3: -0.292\n",
            "w4: 0.078\n",
            "w5: -1.154\n",
            "w6: 0.228\n",
            "Epoch 57 loss: 0.250\n",
            "w1: -1.342\n",
            "w2: 0.121\n",
            "w3: -0.289\n",
            "w4: 0.072\n",
            "w5: -1.172\n",
            "w6: 0.225\n",
            "Epoch 58 loss: 0.250\n",
            "w1: -1.354\n",
            "w2: 0.140\n",
            "w3: -0.286\n",
            "w4: 0.067\n",
            "w5: -1.190\n",
            "w6: 0.223\n",
            "Epoch 59 loss: 0.250\n",
            "w1: -1.367\n",
            "w2: 0.160\n",
            "w3: -0.283\n",
            "w4: 0.062\n",
            "w5: -1.209\n",
            "w6: 0.221\n",
            "Epoch 60 loss: 0.250\n",
            "w1: -1.380\n",
            "w2: 0.181\n",
            "w3: -0.281\n",
            "w4: 0.057\n",
            "w5: -1.228\n",
            "w6: 0.219\n",
            "Epoch 61 loss: 0.250\n",
            "w1: -1.393\n",
            "w2: 0.201\n",
            "w3: -0.278\n",
            "w4: 0.052\n",
            "w5: -1.247\n",
            "w6: 0.217\n",
            "Epoch 62 loss: 0.250\n",
            "w1: -1.406\n",
            "w2: 0.223\n",
            "w3: -0.275\n",
            "w4: 0.046\n",
            "w5: -1.267\n",
            "w6: 0.216\n",
            "Epoch 63 loss: 0.250\n",
            "w1: -1.418\n",
            "w2: 0.244\n",
            "w3: -0.272\n",
            "w4: 0.041\n",
            "w5: -1.287\n",
            "w6: 0.215\n",
            "Epoch 64 loss: 0.250\n",
            "w1: -1.431\n",
            "w2: 0.266\n",
            "w3: -0.269\n",
            "w4: 0.036\n",
            "w5: -1.308\n",
            "w6: 0.214\n",
            "Epoch 65 loss: 0.250\n",
            "w1: -1.444\n",
            "w2: 0.289\n",
            "w3: -0.267\n",
            "w4: 0.031\n",
            "w5: -1.329\n",
            "w6: 0.214\n",
            "Epoch 66 loss: 0.250\n",
            "w1: -1.457\n",
            "w2: 0.312\n",
            "w3: -0.264\n",
            "w4: 0.026\n",
            "w5: -1.350\n",
            "w6: 0.213\n",
            "Epoch 67 loss: 0.250\n",
            "w1: -1.469\n",
            "w2: 0.335\n",
            "w3: -0.261\n",
            "w4: 0.021\n",
            "w5: -1.372\n",
            "w6: 0.213\n",
            "Epoch 68 loss: 0.250\n",
            "w1: -1.482\n",
            "w2: 0.359\n",
            "w3: -0.259\n",
            "w4: 0.016\n",
            "w5: -1.394\n",
            "w6: 0.214\n",
            "Epoch 69 loss: 0.250\n",
            "w1: -1.495\n",
            "w2: 0.383\n",
            "w3: -0.256\n",
            "w4: 0.011\n",
            "w5: -1.417\n",
            "w6: 0.215\n",
            "Epoch 70 loss: 0.250\n",
            "w1: -1.507\n",
            "w2: 0.408\n",
            "w3: -0.253\n",
            "w4: 0.006\n",
            "w5: -1.440\n",
            "w6: 0.215\n",
            "Epoch 71 loss: 0.250\n",
            "w1: -1.520\n",
            "w2: 0.433\n",
            "w3: -0.250\n",
            "w4: 0.001\n",
            "w5: -1.463\n",
            "w6: 0.217\n",
            "Epoch 72 loss: 0.250\n",
            "w1: -1.532\n",
            "w2: 0.459\n",
            "w3: -0.248\n",
            "w4: -0.004\n",
            "w5: -1.487\n",
            "w6: 0.218\n",
            "Epoch 73 loss: 0.250\n",
            "w1: -1.544\n",
            "w2: 0.486\n",
            "w3: -0.245\n",
            "w4: -0.009\n",
            "w5: -1.512\n",
            "w6: 0.220\n",
            "Epoch 74 loss: 0.250\n",
            "w1: -1.557\n",
            "w2: 0.512\n",
            "w3: -0.242\n",
            "w4: -0.015\n",
            "w5: -1.536\n",
            "w6: 0.223\n",
            "Epoch 75 loss: 0.250\n",
            "w1: -1.569\n",
            "w2: 0.540\n",
            "w3: -0.239\n",
            "w4: -0.020\n",
            "w5: -1.562\n",
            "w6: 0.225\n",
            "Epoch 76 loss: 0.250\n",
            "w1: -1.581\n",
            "w2: 0.568\n",
            "w3: -0.236\n",
            "w4: -0.025\n",
            "w5: -1.587\n",
            "w6: 0.228\n",
            "Epoch 77 loss: 0.250\n",
            "w1: -1.593\n",
            "w2: 0.596\n",
            "w3: -0.233\n",
            "w4: -0.031\n",
            "w5: -1.613\n",
            "w6: 0.231\n",
            "Epoch 78 loss: 0.250\n",
            "w1: -1.605\n",
            "w2: 0.625\n",
            "w3: -0.230\n",
            "w4: -0.036\n",
            "w5: -1.640\n",
            "w6: 0.235\n",
            "Epoch 79 loss: 0.250\n",
            "w1: -1.616\n",
            "w2: 0.655\n",
            "w3: -0.227\n",
            "w4: -0.041\n",
            "w5: -1.667\n",
            "w6: 0.239\n",
            "Epoch 80 loss: 0.250\n",
            "w1: -1.628\n",
            "w2: 0.685\n",
            "w3: -0.224\n",
            "w4: -0.047\n",
            "w5: -1.694\n",
            "w6: 0.243\n",
            "Epoch 81 loss: 0.250\n",
            "w1: -1.640\n",
            "w2: 0.716\n",
            "w3: -0.221\n",
            "w4: -0.053\n",
            "w5: -1.722\n",
            "w6: 0.248\n",
            "Epoch 82 loss: 0.250\n",
            "w1: -1.651\n",
            "w2: 0.747\n",
            "w3: -0.217\n",
            "w4: -0.059\n",
            "w5: -1.750\n",
            "w6: 0.253\n",
            "Epoch 83 loss: 0.250\n",
            "w1: -1.662\n",
            "w2: 0.779\n",
            "w3: -0.214\n",
            "w4: -0.064\n",
            "w5: -1.779\n",
            "w6: 0.259\n",
            "Epoch 84 loss: 0.250\n",
            "w1: -1.673\n",
            "w2: 0.812\n",
            "w3: -0.210\n",
            "w4: -0.070\n",
            "w5: -1.808\n",
            "w6: 0.264\n",
            "Epoch 85 loss: 0.250\n",
            "w1: -1.684\n",
            "w2: 0.845\n",
            "w3: -0.207\n",
            "w4: -0.076\n",
            "w5: -1.838\n",
            "w6: 0.271\n",
            "Epoch 86 loss: 0.250\n",
            "w1: -1.695\n",
            "w2: 0.878\n",
            "w3: -0.203\n",
            "w4: -0.083\n",
            "w5: -1.868\n",
            "w6: 0.277\n",
            "Epoch 87 loss: 0.250\n",
            "w1: -1.705\n",
            "w2: 0.913\n",
            "w3: -0.199\n",
            "w4: -0.089\n",
            "w5: -1.898\n",
            "w6: 0.284\n",
            "Epoch 88 loss: 0.250\n",
            "w1: -1.716\n",
            "w2: 0.948\n",
            "w3: -0.195\n",
            "w4: -0.095\n",
            "w5: -1.929\n",
            "w6: 0.291\n",
            "Epoch 89 loss: 0.250\n",
            "w1: -1.726\n",
            "w2: 0.983\n",
            "w3: -0.191\n",
            "w4: -0.102\n",
            "w5: -1.961\n",
            "w6: 0.299\n",
            "Epoch 90 loss: 0.250\n",
            "w1: -1.736\n",
            "w2: 1.019\n",
            "w3: -0.186\n",
            "w4: -0.108\n",
            "w5: -1.993\n",
            "w6: 0.307\n",
            "Epoch 91 loss: 0.250\n",
            "w1: -1.747\n",
            "w2: 1.056\n",
            "w3: -0.182\n",
            "w4: -0.115\n",
            "w5: -2.025\n",
            "w6: 0.316\n",
            "Epoch 92 loss: 0.250\n",
            "w1: -1.757\n",
            "w2: 1.093\n",
            "w3: -0.177\n",
            "w4: -0.122\n",
            "w5: -2.058\n",
            "w6: 0.325\n",
            "Epoch 93 loss: 0.250\n",
            "w1: -1.767\n",
            "w2: 1.130\n",
            "w3: -0.172\n",
            "w4: -0.129\n",
            "w5: -2.091\n",
            "w6: 0.334\n",
            "Epoch 94 loss: 0.250\n",
            "w1: -1.777\n",
            "w2: 1.168\n",
            "w3: -0.167\n",
            "w4: -0.136\n",
            "w5: -2.124\n",
            "w6: 0.344\n",
            "Epoch 95 loss: 0.250\n",
            "w1: -1.787\n",
            "w2: 1.207\n",
            "w3: -0.161\n",
            "w4: -0.143\n",
            "w5: -2.158\n",
            "w6: 0.354\n",
            "Epoch 96 loss: 0.250\n",
            "w1: -1.797\n",
            "w2: 1.246\n",
            "w3: -0.156\n",
            "w4: -0.151\n",
            "w5: -2.193\n",
            "w6: 0.365\n",
            "Epoch 97 loss: 0.250\n",
            "w1: -1.808\n",
            "w2: 1.285\n",
            "w3: -0.150\n",
            "w4: -0.158\n",
            "w5: -2.228\n",
            "w6: 0.376\n",
            "Epoch 98 loss: 0.250\n",
            "w1: -1.818\n",
            "w2: 1.325\n",
            "w3: -0.144\n",
            "w4: -0.166\n",
            "w5: -2.263\n",
            "w6: 0.387\n",
            "Epoch 99 loss: 0.250\n",
            "w1: -1.829\n",
            "w2: 1.365\n",
            "w3: -0.137\n",
            "w4: -0.174\n",
            "w5: -2.299\n",
            "w6: 0.399\n",
            "Epoch 100 loss: 0.250\n",
            "w1: -1.840\n",
            "w2: 1.405\n",
            "w3: -0.131\n",
            "w4: -0.182\n",
            "w5: -2.335\n",
            "w6: 0.411\n",
            "Epoch 101 loss: 0.250\n",
            "w1: -1.851\n",
            "w2: 1.446\n",
            "w3: -0.124\n",
            "w4: -0.189\n",
            "w5: -2.372\n",
            "w6: 0.423\n",
            "Epoch 102 loss: 0.250\n",
            "w1: -1.863\n",
            "w2: 1.486\n",
            "w3: -0.117\n",
            "w4: -0.198\n",
            "w5: -2.409\n",
            "w6: 0.436\n",
            "Epoch 103 loss: 0.250\n",
            "w1: -1.875\n",
            "w2: 1.527\n",
            "w3: -0.109\n",
            "w4: -0.206\n",
            "w5: -2.446\n",
            "w6: 0.449\n",
            "Epoch 104 loss: 0.250\n",
            "w1: -1.887\n",
            "w2: 1.568\n",
            "w3: -0.102\n",
            "w4: -0.214\n",
            "w5: -2.484\n",
            "w6: 0.462\n",
            "Epoch 105 loss: 0.250\n",
            "w1: -1.899\n",
            "w2: 1.608\n",
            "w3: -0.094\n",
            "w4: -0.222\n",
            "w5: -2.522\n",
            "w6: 0.476\n",
            "Epoch 106 loss: 0.250\n",
            "w1: -1.912\n",
            "w2: 1.649\n",
            "w3: -0.085\n",
            "w4: -0.231\n",
            "w5: -2.560\n",
            "w6: 0.490\n",
            "Epoch 107 loss: 0.250\n",
            "w1: -1.926\n",
            "w2: 1.689\n",
            "w3: -0.077\n",
            "w4: -0.239\n",
            "w5: -2.599\n",
            "w6: 0.504\n",
            "Epoch 108 loss: 0.250\n",
            "w1: -1.940\n",
            "w2: 1.729\n",
            "w3: -0.068\n",
            "w4: -0.248\n",
            "w5: -2.638\n",
            "w6: 0.518\n",
            "Epoch 109 loss: 0.250\n",
            "w1: -1.954\n",
            "w2: 1.769\n",
            "w3: -0.059\n",
            "w4: -0.256\n",
            "w5: -2.677\n",
            "w6: 0.533\n",
            "Epoch 110 loss: 0.250\n",
            "w1: -1.969\n",
            "w2: 1.808\n",
            "w3: -0.049\n",
            "w4: -0.265\n",
            "w5: -2.717\n",
            "w6: 0.548\n",
            "Epoch 111 loss: 0.250\n",
            "w1: -1.984\n",
            "w2: 1.847\n",
            "w3: -0.040\n",
            "w4: -0.274\n",
            "w5: -2.756\n",
            "w6: 0.563\n",
            "Epoch 112 loss: 0.000\n",
            "w1: -1.999\n",
            "w2: 1.886\n",
            "w3: -0.030\n",
            "w4: -0.283\n",
            "w5: -2.796\n",
            "w6: 0.578\n",
            "0 0: 0.000\n",
            "1 0: 1.000\n",
            "0 1: 0.000\n",
            "1 1: 0.000\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def deriv_sigmoid(x):\n",
        "  fx = sigmoid(x)\n",
        "  return fx * (1 - fx)\n",
        "\n",
        "def mse_loss(y_true, y_pred):\n",
        "  return ((y_true - y_pred) ** 2).mean()\n",
        "\n",
        "def activate(y_pred):\n",
        "  return round(y_pred)\n",
        "\n",
        "excepted_error = 0.1\n",
        "\n",
        "class OurNeuralNetwork:\n",
        "  def __init__(self):\n",
        "    # Weights\n",
        "    self.w1 = np.random.normal()\n",
        "    self.w2 = np.random.normal()\n",
        "    self.w3 = np.random.normal()\n",
        "    self.w4 = np.random.normal()\n",
        "    self.w5 = np.random.normal()\n",
        "    self.w6 = np.random.normal()\n",
        "\n",
        "    # Biases\n",
        "    self.b1 = np.random.normal()\n",
        "    self.b2 = np.random.normal()\n",
        "    self.b3 = np.random.normal()\n",
        "\n",
        "  def feedforward(self, x):\n",
        "    # x is a numpy array with 2 elements.\n",
        "    h1 = sigmoid(self.w1 * x[0] + self.w2 * x[1] + self.b1)\n",
        "    h2 = sigmoid(self.w3 * x[0] + self.w4 * x[1] + self.b2)\n",
        "    o1 = sigmoid(self.w5 * h1 + self.w6 * h2 + self.b3)\n",
        "    return activate(o1)\n",
        "\n",
        "  def train(self, data, all_y_trues):\n",
        "    learn_rate = 0.5\n",
        "    epochs = 1000 # number of times to loop through the entire dataset\n",
        "    epoch = 1\n",
        "\n",
        "    while True:\n",
        "      for x, y_true in zip(data, all_y_trues):\n",
        "        # --- Do a feedforward (we'll need these values later)\n",
        "        sum_h1 = self.w1 * x[0] + self.w2 * x[1] + self.b1\n",
        "        h1 = sigmoid(sum_h1)\n",
        "\n",
        "        sum_h2 = self.w3 * x[0] + self.w4 * x[1] + self.b2\n",
        "        h2 = sigmoid(sum_h2)\n",
        "\n",
        "        sum_o1 = self.w5 * h1 + self.w6 * h2 + self.b3\n",
        "        o1 = sigmoid(sum_o1)\n",
        "        y_pred = o1\n",
        "\n",
        "        # --- Calculate partial derivatives.\n",
        "        # --- Naming: d_L_d_w1 represents \"partial L / partial w1\"\n",
        "        d_L_d_ypred = -2 * (y_true - y_pred)\n",
        "\n",
        "        # Neuron o1\n",
        "        d_ypred_d_w5 = h1 * deriv_sigmoid(sum_o1)\n",
        "        d_ypred_d_w6 = h2 * deriv_sigmoid(sum_o1)\n",
        "        d_ypred_d_b3 = deriv_sigmoid(sum_o1)\n",
        "\n",
        "        d_ypred_d_h1 = self.w5 * deriv_sigmoid(sum_o1)\n",
        "        d_ypred_d_h2 = self.w6 * deriv_sigmoid(sum_o1)\n",
        "\n",
        "        # Neuron h1\n",
        "        d_h1_d_w1 = x[0] * deriv_sigmoid(sum_h1)\n",
        "        d_h1_d_w2 = x[1] * deriv_sigmoid(sum_h1)\n",
        "        d_h1_d_b1 = deriv_sigmoid(sum_h1)\n",
        "\n",
        "        # Neuron h2\n",
        "        d_h2_d_w3 = x[0] * deriv_sigmoid(sum_h2)\n",
        "        d_h2_d_w4 = x[1] * deriv_sigmoid(sum_h2)\n",
        "        d_h2_d_b2 = deriv_sigmoid(sum_h2)\n",
        "\n",
        "        # --- Update weights and biases\n",
        "        # Neuron h1\n",
        "        self.w1 = self.w1 - (learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w1)\n",
        "        self.w2 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w2\n",
        "        self.b1 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_b1\n",
        "\n",
        "        # Neuron h2\n",
        "        self.w3 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w3\n",
        "        self.w4 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w4\n",
        "        self.b2 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_b2\n",
        "\n",
        "        # Neuron o1\n",
        "        self.w5 -= learn_rate * d_L_d_ypred * d_ypred_d_w5\n",
        "        self.w6 -= learn_rate * d_L_d_ypred * d_ypred_d_w6\n",
        "        self.b3 -= learn_rate * d_L_d_ypred * d_ypred_d_b3\n",
        "\n",
        "      #   print(\"epoch++\")\n",
        "      #   epoch = epoch + 1\n",
        "\n",
        "      # # --- Calculate total loss at the end of each epoch\n",
        "      y_preds = np.apply_along_axis(self.feedforward, 1, data)\n",
        "      loss = mse_loss(all_y_trues, y_preds)\n",
        "      print(\"Epoch %d loss: %.3f\" % (epoch, loss))\n",
        "      print(\"w1: %.3f\" % self.w1)\n",
        "      print(\"w2: %.3f\" % self.w2)\n",
        "      print(\"w3: %.3f\" % self.w3)\n",
        "      print(\"w4: %.3f\" % self.w4)\n",
        "      print(\"w5: %.3f\" % self.w5)\n",
        "      print(\"w6: %.3f\" % self.w6)\n",
        "      epoch = epoch + 1\n",
        "\n",
        "      if(loss <= excepted_error):\n",
        "        break\n",
        "\n",
        "# Define dataset\n",
        "data = np.array([\n",
        "  [0, 0],  \n",
        "  [0, 1],  \n",
        "  [1, 0],   \n",
        "  [1, 1], \n",
        "])\n",
        "all_y_trues = np.array([\n",
        "  0, \n",
        "  0, \n",
        "  1,\n",
        "  0, \n",
        "])\n",
        "\n",
        "network = OurNeuralNetwork()\n",
        "network.train(data, all_y_trues)\n",
        "\n",
        "o_o = np.array([0, 0])\n",
        "l_o = np.array([1, 0])\n",
        "o_l = np.array([0, 1])\n",
        "l_l = np.array([1, 1])\n",
        "print(\"0 0: %.3f\" % network.feedforward(o_o))\n",
        "print(\"1 0: %.3f\" % network.feedforward(l_o))\n",
        "print(\"0 1: %.3f\" % network.feedforward(o_l))\n",
        "print(\"1 1: %.3f\" % network.feedforward(l_l))"
      ]
    }
  ]
}